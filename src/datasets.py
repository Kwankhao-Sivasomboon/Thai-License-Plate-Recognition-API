import torch
from torch.utils.data import Dataset
from PIL import Image, ImageOps
from pathlib import Path
import torchvision.transforms as T
import torchvision.transforms.functional as F
import numpy as np

# --- üåü Custom Transform: Smart Resize with Padding üåü ---
class SmartResize:
    def __init__(self, target_size, fill=0, is_ocr=False):
        """
        target_size: (height, width) for OCR, (height, width) for Province
        is_ocr: ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô OCR ‡πÄ‡∏£‡∏≤‡∏à‡∏∞ fix height ‡πÅ‡∏•‡πâ‡∏ß‡∏õ‡∏•‡πà‡∏≠‡∏¢ width
        """
        self.target_size = target_size # (H, W)
        self.fill = fill
        self.is_ocr = is_ocr

    def __call__(self, img):
        # img is PIL Image
        tgt_h, tgt_w = self.target_size
        w, h = img.size

        # 1. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Scale ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô
        if self.is_ocr:
            # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö OCR: ‡∏¢‡∏∂‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏π‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å (Height=64), ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏≤‡∏°
            scale = tgt_h / h
            new_h = tgt_h
            new_w = int(w * scale)
            # ‡πÅ‡∏ï‡πà‡∏ñ‡πâ‡∏≤ new_w ‡πÄ‡∏Å‡∏¥‡∏ô tgt_w ‡πÉ‡∏´‡πâ‡∏¢‡∏∂‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡πÅ‡∏ó‡∏ô
            if new_w > tgt_w:
                scale = tgt_w / w
                new_w = tgt_w
                new_h = int(h * scale)
        else:
            # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Province (Square): ‡∏¢‡∏∂‡∏î‡∏î‡πâ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏¢‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
            scale = min(tgt_h / h, tgt_w / w)
            new_h = int(h * scale)
            new_w = int(w * scale)

        # 2. Resize ‡∏î‡πâ‡∏ß‡∏¢ BICUBIC (‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏û‡πÄ‡∏•‡πá‡∏Å‡πÑ‡∏õ‡πÉ‡∏´‡∏ç‡πà)
        img = img.resize((new_w, new_h), resample=Image.BICUBIC)

        # 3. Create Background & Paste (Padding)
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏†‡∏≤‡∏û‡∏û‡∏∑‡πâ‡∏ô‡∏´‡∏•‡∏±‡∏á‡∏™‡∏µ‡∏î‡∏≥ (‡∏´‡∏£‡∏∑‡∏≠‡∏™‡∏µ‡πÄ‡∏ó‡∏≤‡∏Ñ‡πà‡∏≤ 0)
        # ‡∏ñ‡πâ‡∏≤‡∏†‡∏≤‡∏û‡πÄ‡∏î‡∏¥‡∏°‡πÄ‡∏õ‡πá‡∏ô L (Gray) ‡∏û‡∏∑‡πâ‡∏ô‡∏´‡∏•‡∏±‡∏á‡∏Å‡πá L, ‡∏ñ‡πâ‡∏≤ RGB ‡∏û‡∏∑‡πâ‡∏ô‡∏´‡∏•‡∏±‡∏á‡∏Å‡πá RGB
        new_img = Image.new(img.mode, (tgt_w, tgt_h), self.fill)
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ß‡∏≤‡∏á‡∏ï‡∏£‡∏á‡∏Å‡∏•‡∏≤‡∏á
        paste_x = (tgt_w - new_w) // 2
        paste_y = (tgt_h - new_h) // 2
        
        new_img.paste(img, (paste_x, paste_y))
        return new_img

# --- Transforms Config ---
def get_ocr_transforms(is_train=True):
    # OCR Target: Height=64, Width=256 (‡∏ï‡∏≤‡∏°‡πÇ‡∏°‡πÄ‡∏î‡∏• CRNN)
    base_transforms = [
        # ‡πÉ‡∏ä‡πâ Smart Resize ‡πÅ‡∏ó‡∏ô T.Resize((64, 256)) ‡πÄ‡∏î‡∏¥‡∏°
        SmartResize((64, 256), is_ocr=True),
        T.ToTensor(),
        T.Normalize(mean=[0.5], std=[0.5])
    ]
    
    if is_train:
        # Augmentation ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö OCR
        augments = [
            T.RandomApply([T.GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 1.0))], p=0.3),
            # ‡∏ï‡∏±‡∏î ColorJitter ‡∏≠‡∏≠‡∏Å‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÄ‡∏õ‡πá‡∏ô Grayscale
            T.RandomAffine(degrees=2, translate=(0.02, 0.05), shear=5, fill=0),
        ]
        return T.Compose(augments + base_transforms)
    else:
        return T.Compose(base_transforms)

def get_prov_transforms(is_train=True):
    # Province Target: 224x224 (‡∏´‡∏£‡∏∑‡∏≠‡∏•‡∏î‡πÄ‡∏´‡∏•‡∏∑‡∏≠ 128x128 ‡∏Å‡πá‡πÑ‡∏î‡πâ‡∏ñ‡πâ‡∏≤‡∏†‡∏≤‡∏û‡πÅ‡∏ï‡∏Å‡∏°‡∏≤‡∏Å)
    # ‡πÅ‡∏ï‡πà MobileNetV2 ‡∏õ‡∏Å‡∏ï‡∏¥‡∏£‡∏±‡∏ö 224
    
    # üåü FIX: ‡∏£‡∏±‡∏ö Grayscale ‡πÅ‡∏ï‡πà‡∏ó‡∏≥‡πÄ‡∏õ‡πá‡∏ô 3 Channels (Fake RGB) ‡πÉ‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏•‡∏¢
    # ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ Channel Mismatch ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏Å‡πâ Dataset
    
    ops = []
    
    # 1. Smart Resize (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏û‡πÄ‡∏•‡πá‡∏Å 41x12)
    ops.append(SmartResize((224, 224), is_ocr=False))
    
    # 2. Augmentation (Train only)
    if is_train:
        ops.append(T.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.8, 1.1), shear=10))
        ops.append(T.RandomPerspective(distortion_scale=0.2, p=0.3))
    
    # 3. Convert to Tensor & Normalize
    ops.append(T.ToTensor())
    
    # 4. üåü ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: ‡∏ñ‡πâ‡∏≤‡∏†‡∏≤‡∏û‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô 1 Channel (Gray) ‡πÉ‡∏´‡πâ‡∏Å‡πä‡∏≠‡∏õ‡∏õ‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô 3 Channels
    # Lambda function ‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô 1 channel ‡πÉ‡∏´‡πâ‡∏ó‡∏≥‡∏ã‡πâ‡∏≥
    ops.append(T.Lambda(lambda x: x.repeat(3, 1, 1) if x.shape[0] == 1 else x))
    
    # Normalize (‡∏Ñ‡πà‡∏≤‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô ImageNet)
    ops.append(T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]))
    
    return T.Compose(ops)

# --- Datasets (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏Å‡πâ Logic ‡∏°‡∏≤‡∏Å ‡πÅ‡∏Ñ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ) ---
class OCRDataset(Dataset):
    def __init__(self, df, root, char_to_int, transform=None):
        self.df = df.reset_index(drop=True)
        self.root = Path(root)
        self.cti = char_to_int
        self.transform = transform

    def encode(self, txt):
        txt = str(txt) if txt is not None else ""
        return torch.tensor([self.cti[c] for c in txt if c in self.cti], dtype=torch.long)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        img_path = self.root / row["image"]
        try:
            # ‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏õ‡πá‡∏ô Grayscale (L) ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
            img = Image.open(img_path).convert("L")
        except:
            img = Image.new('L', (256, 64))

        if self.transform:
            img = self.transform(img)

        target = self.encode(row["gt_plate"])
        return img, target, len(target), row["gt_plate"], str(img_path)

    def __len__(self): return len(self.df)

class ProvinceDataset(Dataset):
    def __init__(self, df, root, class_map=None, training=True):
        self.df = df.reset_index(drop=True)
        self.root = Path(root)
        self.training = training
        
        if class_map is not None:
            self.p2i = class_map
            self.i2p = {i:p for p,i in self.p2i.items()}
        else:
            self.provs = sorted(df["gt_province"].unique())
            self.p2i = {p:i for i,p in enumerate(self.provs)}
            self.i2p = {i:p for p,i in self.p2i.items()}

        self.transform = get_prov_transforms(training)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        img_rel_plate = row["image"]
        # Logic ‡πÅ‡∏õ‡∏•‡∏á path ‡πÑ‡∏õ‡∏´‡∏≤‡∏†‡∏≤‡∏û‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î
        img_rel_prov = img_rel_plate.replace("/plates/", "/provs/").replace("__plate", "__prov")
        img_path = self.root / img_rel_prov

        try:
            # ‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏õ‡πá‡∏ô Grayscale (L) ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏° ‡πÅ‡∏•‡πâ‡∏ß‡πÉ‡∏´‡πâ Transform ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô Fake RGB
            img = Image.open(img_path).convert("L") 
        except:
            img = Image.new("L", (224, 224))

        img = self.transform(img)
        label = self.p2i.get(row["gt_province"], 0)
        return img, torch.tensor(label, dtype=torch.long)

    def __len__(self): return len(self.df)

def ocr_collate(batch):
    imgs, tg, lens, texts, names = zip(*batch)
    return torch.stack(imgs), torch.cat(tg), torch.tensor(lens, dtype=torch.long), None, texts, names